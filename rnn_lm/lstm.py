from __future__ import print_function
from datetime import datetime
import tensorflow as tf
import numpy as np
import os
import sys
import thread_ops
from data_handler import DataHandler, char_to_vector, vector_to_char, string_to_tensor



# num_hidden_units - Number of hidden neurons in the LSTM cell.
# batch_size - The number of training data vectors to feed to the network at
#   a time.
# num_lstm_layers - Number of LSTM cells to have.
num_hidden_units = 256
num_lstm_layers = 2
batch_size = 64


# Converted this process from a regular method to a class so that object properties
# can be taken advantage of.
class RNN(object):

	def __init__(self, session, scope_name, vocab_size):

		self.session = session
		params = np.load('charembedding.npz')

		with tf.variable_scope(scope_name):

			static_weights = tf.constant(params['arr_0'])
			static_biases = tf.constant(params['arr_1'])

			self.in_weights = tf.get_variable(name='utw', trainable=False, initializer=static_weights)
			self.in_biases = tf.get_variable(name='utb', trainable=False, initializer=static_biases)

			self.out_weights = tf.Variable(tf.random_normal((num_hidden_units, vocab_size), stddev=0.01))
			self.out_biases = tf.Variable(tf.random_normal((vocab_size,), stddev=0.01))

			# Declaring this as a placeholder means that it must be fed with data 
			# at execution time.
			self.feed_x = tf.placeholder(dtype=tf.float32, shape=(None, None, vocab_size))
			self.feed_y = tf.placeholder(dtype=tf.float32, shape=(None, None, vocab_size))
			self.feed_lr = tf.placeholder(dtype=tf.float32, shape=(None))
			
			self.hidden_states = []
			for i in range(num_lstm_layers):
				temp_placeholder_1 = tf.placeholder(dtype=tf.float32, shape=(None, num_hidden_units))
				temp_placeholder_2 = tf.placeholder(dtype=tf.float32, shape=(None, num_hidden_units))
				self.hidden_states.append([temp_placeholder_1, temp_placeholder_2])

			self.rnn_tuple_states = []
			for i in range(num_lstm_layers):
				self.rnn_tuple_states.append(tf.contrib.rnn.LSTMStateTuple(self.hidden_states[i][0], self.hidden_states[i][1]))
			self.rnn_tuple_states = tuple(self.rnn_tuple_states)

			# Assume that every LSTM cell in the network has the same number of hidden nodes.
			self.lstm_cell = [tf.contrib.rnn.BasicLSTMCell(num_units=num_hidden_units, forget_bias=1.0) for _ in range(num_lstm_layers)]

			# MultiRNNCell - allows multiple recurrent cells to be stacked on top of 
			# each other. Note that we are explicitly duplicating the structure of
			# each cell.
			self.multi_lstm = tf.contrib.rnn.MultiRNNCell(self.lstm_cell)
			
			self.lstm_zero_states = self.multi_lstm.zero_state(batch_size, dtype=tf.float32)

			num_tiles = tf.shape(self.feed_x)[0]
			expanded_weights = tf.expand_dims(self.in_weights, axis=0)
			tiled_weights = tf.tile(expanded_weights, tf.stack([num_tiles, 1, 1]))

			embedded_input = tf.tanh(tf.matmul(self.feed_x, tiled_weights) + self.in_biases)


			self.outputs, self.last_lstm_state = tf.nn.dynamic_rnn(cell=self.multi_lstm, inputs=embedded_input, initial_state=self.rnn_tuple_states, dtype=tf.float32)

			local_field = tf.matmul(tf.reshape(self.outputs, [-1, num_hidden_units]), self.out_weights) + self.out_biases

			feed_y_long = tf.reshape(self.feed_y, [-1, vocab_size])
			self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=local_field, labels=feed_y_long))

			outputs_shape = tf.shape(self.outputs)
			# Convert the local fields generated by the output of the LSTM into a softmax output.
			self.softmax_output = tf.reshape(tf.nn.softmax(local_field), (outputs_shape[0], outputs_shape[1], vocab_size))

			# Declare the optimizer and the op that you want to minimize.
			self.train_operation = tf.train.RMSPropOptimizer(learning_rate=self.feed_lr).minimize(self.cost)

	def train(self, batch_x, batch_y, lr=0.001):
		
		zero_states = self.session.run(self.lstm_zero_states)
		#print('shape of zero states:', type(zero_states), type(zero_states[0]), zero_states[0][0].shape)
		#zero_states = np.zeros((num_lstm_layers, 2, batch_size, num_hidden_units))
		#zero_state = np.zeros(1, num_hidden_units)
		feeds = {
			self.feed_x:batch_x,
			self.feed_y:batch_y,
			self.feed_lr:lr
		}
		for i in range(num_lstm_layers):
			feeds[self.hidden_states[i][0]] = zero_states[i][0]
			feeds[self.hidden_states[i][1]] = zero_states[i][1]

		fetches = [
			self.cost,
			self.train_operation
		]
		
		#feed_dict[self.hidden_states] = zero_states
		#print('shape of x, y feeds', batch_x.shape, batch_y.shape)
		#print('shape of zero_states:', zero_states[i][0].shape, zero_states[i][1].shape)
		cost, _ = self.session.run(fetches, feed_dict=feeds)
		
		return cost

	def test(self, valid_x, valid_y):
		'''
		Runs the network on validation data and produces the perplexity of a
		particular phrase.
		'''
		zero_states = self.session.run(self.lstm_zero_states)

		feed_dict = {}
		feed_dict[self.feed_x] = valid_x
		feed_dict[self.feed_y] = valid_y

		for i in range(num_lstm_layers):
			feed_dict[self.hidden_states[i][0]] = zero_states[i][0]
			feed_dict[self.hidden_states[i][1]] = zero_states[i][1]

		cost = self.session.run(self.cost, feed_dict=feed_dict)
		perplexity = np.exp(cost)
		return perplexity

	def run(self, x, vector_to_char, char_to_vector, num_steps=25, delimiter=None):
		'''
		This method will create 'num_steps' unrollings of the network after passing
		the input tensor, 'x', to the network.
		'''
		test_output = ''
		#zero_state = self.session.run(self.multi_lstm.zero_state(x.shape[0], dtype=tf.float32))
		zero_states = np.zeros((num_lstm_layers, 2, 1, num_hidden_units))
		lstm_next_state = zero_states
		#feed_dict={self.feed_x:x, self.state_r:zero_state_r, self.state_c:zero_state_c}
		feed_dict={}

		for i in range(x.shape[0]):
			feed_dict[self.feed_x] = [[x[i]]]
			#feed_dict[self.hidden_states] = lstm_next_state
			for j in range(num_lstm_layers):
				feed_dict[self.hidden_states[j][0]] = lstm_next_state[j][0]
				feed_dict[self.hidden_states[j][1]] = lstm_next_state[j][1]
				#print(str(lstm_next_state[j][0]))
				#print(str(lstm_next_state[j][1]))
			softmax_out, lstm_state_out = self.session.run([self.softmax_output, self.last_lstm_state], feed_dict=feed_dict)
			lstm_next_state = lstm_state_out
			#print('lstm state type, size:', type(lstm_state_out), lstm_state_out[0][0].shape)
			char_out = vector_to_char(softmax_out[0][0])
			test_output += char_out
			
		for i in range(num_steps):
			lstm_in = char_to_vector(char_out)
			lstm_next_state = lstm_state_out

			feed_dict[self.feed_x] = [[lstm_in]]
			#feed_dict[self.hidden_states] = lstm_next_state
			for j in range(num_lstm_layers):
				feed_dict[self.hidden_states[j][0]] = lstm_next_state[j][0]
				feed_dict[self.hidden_states[j][1]] = lstm_next_state[j][1]
			softmax_out, lstm_state_out = self.session.run([self.softmax_output, self.last_lstm_state], feed_dict=feed_dict)	
			#lstm_next_state = lstm_state_out
			char_out = vector_to_char(softmax_out[0][0])
			test_output += char_out

		return test_output

def main(argv):
	load_checkpoint = False
	
	perplexity = []
	halving_threshold = 0.003
	
	max_plateaus = 15
	max_steps = 10000
	sample_step_percentage = .01
	sample_weight_percentage = 0.005
	num_timesteps = 100
	print_steps = False

	date = datetime.now()
	date = str(date).replace(' ', '')
	date = date.replace(':', '')

	if len(argv) > 0:
		if argv[0] == 'load':
			load_checkpoint = True
			checkpoint_file = argv[0]
		elif argv[0] == 'print':
			print_steps = True

	# Start tf session. This is passed to the RNN class.
	sess = tf.Session()

	cud = os.getcwd()
	data_dir = os.path.join(cud, 'data', '')
	save_dir = os.path.join(cud, 'saves', '')
	error_dir = os.path.join(cud, 'training_error', '')
	weight_saver = thread_ops.weightThread()
	shakespeare = DataHandler('shakespeare.train.txt', 'shakespeare.test.txt', 'shakespeare.valid.txt', data_dir=data_dir)
	penntreebank = DataHandler('ptb.train.txt', 'ptb.test.txt', 'ptb.valid.txt', data_dir=data_dir)
	#merge_vocabularies(shakespeare, penntreebank)
	vtoc = vector_to_char
	ctov = char_to_vector

	network = RNN(sess, 'stuff', shakespeare.vocab_size)

	sess.run(tf.global_variables_initializer())
	saver = tf.train.Saver()

	if load_checkpoint and os.path.isfile(save_dir + checkpoint_file):
		saver.restore(sess, save_dir + 'lstmsmall.ckpt')
		print(network.run(string_to_tensor('random stuff'), num_steps=1500))

	else:
		# Retrieve the list of trainable variables. The goal is to save these
		# weights as a grayscale PNG to show how the values evolve over time.
		trainable_vars = tf.trainable_variables()

		for data in (penntreebank, shakespeare):
			step = 1
			num_plateaus = 0
			learning_rate = 0.1
			
			while step < max_steps and num_plateaus < max_plateaus:
				# The training batch has to have specific dimensions:
				#   (batch_size, num_timesteps, vocab_length)
				train_x, train_y = data.get_random_batch(num_timesteps, batch_size, 'train')

				
				if print_steps:
					print(step)
				# Note that the elements of the feed dictionary are the placeholders
				# defined earlier in the program.
				cost = network.train(train_x, train_y, lr=learning_rate)
				
				if step % int(sample_step_percentage*float(max_steps)) == 0:
					valid_x, valid_y = data.get_random_batch(num_timesteps, batch_size, 'validation')
					ppl = network.test(valid_x, valid_y)
					perplexity.append(ppl)

					valid_x, valid_y = shakespeare.get_random_batch(num_timesteps, batch_size, 'validation')
					ppl1 = network.test(valid_x, valid_y)
					valid_x, valid_y = penntreebank.get_random_batch(num_timesteps, batch_size, 'validation')
					ppl2 = network.test(valid_x, valid_y)
					
					training_output = network.run(string_to_tensor('the '), vtoc, ctov, num_steps=500)

					print('-----------------------------------------------------')
					print(float(step)*100.0/float(max_steps), 'percent complete')
					print('current step', step, 'out of', max_steps)
					print('\tcost:', cost)
					print('\tset perplexity:', ppl)
					print('\tshakespeare perplexity:', ppl1)
					print('\tpenn treebank perplexity:', ppl2)
					#print('\tnumber of weight images saved:', weight_saver.iteration)

					# If the perplexity doesn't change by more than the threshold
					# halve the learning rate. We assume that a training plateau
					# is reached if the learning rate is halved.
					if len(perplexity) > 1:
						dp = perplexity[-2] - perplexity[-1]
						dp /= perplexity[-2]
						print('\tdelta_perplexity:', dp)
						if dp < halving_threshold:
							learning_rate /= 2.
							num_plateaus += 1
							print('\t\tlearning rate halved', learning_rate)
							print('\t\tnumber of plateaus reached:', num_plateaus)
						dp = 0
					print('training output:\n', training_output)
					
					with open(error_dir + date + 'train_errors.txt', 'a') as f:
						f.write(str(ppl1) + ',' + str(ppl2) + '\n')

				#if step % int(sample_weight_percentage*float(max_steps)) == 0:
					
					#weights = sess.run(trainable_vars)
					#weight_saver.run(weights)

				step += 1

		saver.save(sess, save_dir + date + 'lstmsmall.ckpt')
	print(network.run(string_to_tensor('random stuff'), vtoc, ctov, num_steps=500))

if __name__ == '__main__':
	main(sys.argv[1:])
